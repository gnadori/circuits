<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Face Detection Test v3</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- TensorFlow.js Core -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core@3.11.0/dist/tf-core.min.js"></script>
    <!-- TensorFlow.js Converter -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter@3.11.0/dist/tf-converter.min.js"></script>
    <!-- TensorFlow.js Backend WebGL -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl@3.11.0/dist/tf-backend-webgl.min.js"></script>
    
    <!-- NEW MODEL: MediaPipe Face Detector -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_detection@0.4/face_detection.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-detection@1.0.1/dist/face-detection.min.js"></script>
    
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        /* REMOVED: transform: scaleX(-1) from canvas. We will do this in JS now. */
    </style>
</head>
<body class="bg-gray-900 text-white flex flex-col items-center justify-center min-h-screen p-4">

    <div class="w-full max-w-2xl mx-auto">
        <h1 class="text-2xl font-bold text-center mb-4">Face Detection Test (New Model)</h1>
        
        <div id="loading" class="text-center p-4">
            <div class="inline-block animate-spin rounded-full h-8 w-8 border-t-2 border-b-2 border-blue-500 mb-2"></div>
            <p>Loading Model & Starting Camera...</p>
        </div>

        <div class="relative w-full rounded-lg overflow-hidden shadow-lg bg-gray-800" style="padding-top: 75%;">
            <canvas id="canvas" class="absolute top-0 left-0 w-full h-full"></canvas>
            <video id="video" playsinline style="display: none;"></video>
        </div>
        
        <!-- Camera Control Buttons -->
        <div class="grid grid-cols-2 gap-4 mt-4">
            <button id="startButton" class="w-full bg-blue-600 hover:bg-blue-700 text-white font-bold py-3 px-4 rounded-lg transition duration-300">
                Start Camera
            </button>
            <button id="switchButton" class="w-full bg-gray-600 hover:bg-gray-700 text-white font-bold py-3 px-4 rounded-lg transition duration-300" style="display: none;">
                Switch Camera
            </button>
        </div>
    </div>

    <script>
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        const loading = document.getElementById('loading');
        const startButton = document.getElementById('startButton');
        const switchButton = document.getElementById('switchButton');

        let detector; // Changed from 'model'
        let currentFacingMode = 'user'; // 'user' (front) or 'environment' (back)

        async function setupCamera() {
            loading.innerText = "Accessing camera...";
            // Stop any existing stream
            if (video.srcObject) {
                video.srcObject.getTracks().forEach(track => track.stop());
            }

            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    video: { facingMode: currentFacingMode },
                    audio: false
                });
                video.srcObject = stream;
                return new Promise((resolve) => {
                    video.onloadedmetadata = () => {
                        resolve(video);
                    };
                });
            } catch (err) {
                console.error("Error accessing camera: ", err);
                loading.innerText = "Error accessing camera. Please grant permission and refresh.";
            }
        }

        async function loadModel() {
            loading.innerText = "Loading detection model...";
            try {
                await tf.setBackend('webgl');
                const model = faceDetection.SupportedModels.MediaPipeFaceDetector;
                const detectorConfig = {
                    runtime: 'tfjs', // or 'mediapipe'
                    // solutionPath: 'https://cdn.jsdelivr.net/npm/@mediapipe/face_detection@0.4'
                };
                detector = await faceDetection.createDetector(model, detectorConfig);
                loading.innerText = "Model loaded.";
            } catch (err) {
                console.error("Error loading model: ", err);
                loading.innerText = "Error loading model. Please refresh.";
            }
        }

        async function detectFaces() {
            if (!detector || !video.videoWidth) {
                requestAnimationFrame(detectFaces);
                return;
            }

            // Set canvas size to match video
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;

            // --- Draw mirrored video ---
            // This is the proper way to mirror the video for a "selfie" view
            ctx.save();
            ctx.scale(-1, 1);
            ctx.translate(-canvas.width, 0);
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
            ctx.restore();
            // --- End mirrored draw ---

            // [NEW] Add a red "heartbeat" square (drawn *after* restore, so it's not mirrored)
            ctx.fillStyle = 'red';
            ctx.fillRect(10, 10, 15, 15);

            // 2. Detect faces (using the non-mirrored video element for better accuracy)
            const predictions = await detector.estimateFaces(video);

            // 3. Draw boxes
            if (predictions.length > 0) {
                for (let i = 0; i < predictions.length; i++) {
                    const box = predictions[i].box;
                    const x = box.xMin;
                    const y = box.yMin;
                    const width = box.width;
                    const height = box.height;
                    
                    // We must flip the X coordinate for drawing on the mirrored canvas
                    const mirroredX = canvas.width - x - width;

                    // 4. Draw a green box
                    ctx.strokeStyle = 'rgba(0, 255, 0, 0.8)';
                    ctx.lineWidth = 4;
                    ctx.strokeRect(mirroredX, y, width, height);
                }
            }

            // 5. Repeat
            requestAnimationFrame(detectFaces);
        }

        // --- Event Listeners ---
        
        startButton.addEventListener('click', async () => {
            startButton.style.display = 'none';
            switchButton.style.display = 'block';
            loading.style.display = 'block';
            
            await loadModel();
            await setupCamera();
            video.play();
            
            loading.style.display = 'none';
            detectFaces();
        });

        switchButton.addEventListener('click', async () => {
            // Toggle facing mode
            currentFacingMode = (currentFacingMode === 'user') ? 'environment' : 'user';
            
            loading.style.display = 'block';
            loading.innerText = "Switching camera...";
            
            // Restart the camera
            await setupCamera();
            video.play();
            
            loading.style.display = 'none';
        });

    </script>
</body>
</html>

